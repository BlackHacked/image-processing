{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCUDA-卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Matrix Convolution in PyCUDA\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "# np.set_printoptions(threshold=np.nan)\n",
    "np.set_printoptions(edgeitems=8)\n",
    "#import PYCUDA modules and libraries\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "import sys\n",
    "#the following module is used to mark the time stamps\n",
    "import time\n",
    "#import necessary scipy libraries\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "from scipy.signal import convolve2d as conv2d\n",
    "\n",
    "# -- initialize the device\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_code_template = \"\"\"\n",
    "//2D Convolution function\n",
    "__global__ void convolve2d(int* A, int* K,\n",
    "                           const int M, const int N,\n",
    "                           const int F, int* C)\n",
    "{\n",
    "    #ifndef TILE_M\n",
    "        #define TILE_M 3u\n",
    "    #endif\n",
    "    #ifndef TILE_N\n",
    "        #define TILE_N 3u\n",
    "    #endif\n",
    "    // A - input size MxN\n",
    "    // K - Kernel size FxF\n",
    "    // M - Row Dim of UN-Padded A\n",
    "    // N - Column Dim of UN-Padded A\n",
    "    // F - Square Dim of K (Pitch)\n",
    "    // C - Output size MxN\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int i = blockIdx.x * blockDim.x + tx;\n",
    "    int j = blockIdx.y * blockDim.y + ty;\n",
    "    if(i < M && j <  N){\n",
    "    // check to make sure we are within bounds of the overall output size\n",
    "        // create flattened padded matrix size M+2 x N+2 to use for convolution input\n",
    "        extern __shared__ int DS_A_PAD[];\n",
    "        int M_LIM = (M > 32) ? ((blockIdx.x+1)*blockDim.x < M) ? blockDim.x : blockDim.x - (blockIdx.x+1)*blockDim.x % M : M;\n",
    "        int N_LIM = (N > 32) ? ((blockIdx.y+1)*blockDim.y < N) ? blockDim.y : blockDim.y - (blockIdx.y+1)*blockDim.y % N : N;\n",
    "        //// ZERO PADDING - comes before fill so as not to overwrite\n",
    "        for(int k = 0; k < M_LIM + F-1; k++){\n",
    "            for(int l = 0; l < N_LIM + F-1; l++){\n",
    "                DS_A_PAD[k*(N_LIM+F-1) + l] = 0;\n",
    "            }\n",
    "        }\n",
    "        __syncthreads();\n",
    "        //// FILL VALUES OF DS_A_PAD\n",
    "        bool left = ( ty == 0 ) ? true : false;\n",
    "        bool right = ( ty == N_LIM-1 ) ? true : false;\n",
    "        bool top = ( tx == 0 ) ? true : false;\n",
    "        bool bottom = ( tx == M_LIM-1 ) ? true : false;\n",
    "        if ( left && j != 0 ) {\n",
    "        // vertical left block edges - CHECK!\n",
    "            for(int z=1; z <= F/2; z++){\n",
    "                if ((j-z) < 0){\n",
    "                    DS_A_PAD[(tx+F/2)*(N_LIM+F-1)+(ty+F/2-z)] = 0;\n",
    "                } else {\n",
    "                    DS_A_PAD[(tx+F/2)*(N_LIM+F-1)+(ty+F/2-z)] = A[i*N + (j-z)];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        if ( right && j != N-1 ) {\n",
    "        // veritcal right block edges - WATCH!\n",
    "            for(int z=1; z <= F/2; z++){\n",
    "                if ((j+z) >= N){\n",
    "                    DS_A_PAD[(tx+F/2)*(N_LIM+F-1)+(ty+F/2+z)] = 0;\n",
    "                } else {\n",
    "                    DS_A_PAD[(tx+F/2)*(N_LIM+F-1)+(ty+F/2+z)] = A[i*N + (j+z)];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        if ( top && i != 0 ) {\n",
    "        // horizontal top block edges - CHECK!\n",
    "            for(int z=1; z <= F/2; z++){\n",
    "                if((i-z) < 0){\n",
    "                    DS_A_PAD[(tx+F/2-z)*(N_LIM+F-1)+(ty+F/2)] = 0;\n",
    "                } else {\n",
    "                    DS_A_PAD[(tx+F/2-z)*(N_LIM+F-1)+(ty+F/2)] = A[(i-z)*N + j];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        if ( bottom && i != M-1) {\n",
    "        // horizontal bottom block edges - WATCH!\n",
    "            for(int z=1; z <= F/2; z++){\n",
    "                if ((i+z) >= M){\n",
    "                    DS_A_PAD[(tx+F/2+z)*(N_LIM+F-1)+(ty+F/2)] = 0;\n",
    "                } else {\n",
    "                    DS_A_PAD[(tx+F/2+z)*(N_LIM+F-1)+(ty+F/2)] = A[(i+z)*N + j];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        if ( bottom && right && j != N-1 && i != M-1 ) {\n",
    "        // lower righthand block corners (outside current block) - WATCH!\n",
    "            for(int q=1; q <= F/2; q++){\n",
    "                for(int z=1; z <= F/2; z++){\n",
    "                    if((i+z) >= M || (j+q) >= N){\n",
    "                        DS_A_PAD[(tx+F/2+z)*(N_LIM+F-1)+(ty+F/2+q)] = 0;\n",
    "                    } else {\n",
    "                        DS_A_PAD[(tx+F/2+z)*(N_LIM+F-1)+(ty+F/2+q)] = A[(i+z)*N + (j+q)];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        if ( bottom && left && j != 0 && i < M-1 ) {\n",
    "        // lower lefthand block corners (outside current block) - WATCH!\n",
    "            for(int q=1; q <= F/2; q++){\n",
    "                for(int z=1; z <= F/2; z++){\n",
    "                    if((i+z) >= M || (j+q) < 0){\n",
    "                        DS_A_PAD[(tx+F/2+z)*(N_LIM+F-1)+(ty+F/2-q)] = 0;\n",
    "                    } else {\n",
    "                        DS_A_PAD[(tx+F/2+z)*(N_LIM+F-1)+(ty+F/2-q)] = A[(i+z)*N + (j-q)];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        if ( top && right && i != 0 && j < N-1 ) {\n",
    "        // upper righthand block corners (outside current block) - CHECK!\n",
    "            for(int q=1; q <= F/2; q++){\n",
    "                for(int z=1; z <= F/2; z++){\n",
    "                    if((i-z) >= M || (j+q) >= N){\n",
    "                        DS_A_PAD[(tx+F/2-z)*(N_LIM+F-1)+(ty+F/2+q)] = 0;\n",
    "                    } else {\n",
    "                        DS_A_PAD[(tx+F/2-z)*(N_LIM+F-1)+(ty+F/2+q)] = A[(i-z)*N + (j+q)];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        if ( top && left && i != 0 && j != 0 ) {\n",
    "        // upper lefthand block corners (outside current block)\n",
    "            for(int q=1; q <= F/2; q++){\n",
    "                for(int z=1; z <= F/2; z++){\n",
    "                    if((i-z) < 0 || (j-q) <0){\n",
    "                        DS_A_PAD[(tx+F/2-z)*(N_LIM+F-1)+(ty+F/2-q)] = 0;\n",
    "                    } else {\n",
    "                        DS_A_PAD[(tx+F/2-z)*(N_LIM+F-1)+(ty+F/2-q)] = A[(i-z)*N + (j-q)];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        // internal elements\n",
    "            DS_A_PAD[(tx+F/2)*(N_LIM+F-1)+(ty+F/2)] = A[i*N + j];\n",
    "        __syncthreads();\n",
    "        //C[i*N+j] = DS_A_PAD[(tx+F/2-2)*(N_LIM+F-1) + (ty + F/2-2)];\n",
    "        //// CONVOLUTION Calculation for element (i,j)\n",
    "        C[i*N + j] = 0;\n",
    "        for(int m = 0; m < F; m++){\n",
    "            for(int n = 0; n < F; n++){\n",
    "                C[i*N + j] += K[m*F + n] * DS_A_PAD[(tx-m+F-1)*(N_LIM+F-1) +(ty-n+F-1)];\n",
    "            }\n",
    "        }// end convolution calculation\n",
    "        __syncthreads();\n",
    "    }// end output boundary check\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_instance(M,N,F,a,f,c):\n",
    "\n",
    "    # get the kernel code from the template\n",
    "    kernel_code = kernel_code_template\n",
    "\n",
    "    # compile the kernel code, with options\n",
    "    TILE_M = str(M) + 'u' if (M <= 32) else str(32) + 'u'\n",
    "    TILE_N = str(N) + 'u' if (N <= 32) else str(32) + 'u'\n",
    "    options = \"-DTILE_M=\" + TILE_M + \" -DTILE_N=\" + TILE_N\n",
    "    OPTIONS = [_flag.strip() for _flag in options.split() if _flag.strip()]\n",
    "    # print OPTIONS\n",
    "    mod = compiler.SourceModule(kernel_code,options=OPTIONS)\n",
    "\n",
    "    # get the kernel function from the compiled module\n",
    "    conv = mod.get_function(\"convolve2d\")\n",
    "\n",
    "    # create buffers for transfer into GPU\n",
    "    A_buf = gpuarray.to_gpu(a)\n",
    "    K_buf = gpuarray.to_gpu(f)\n",
    "    C_buf = gpuarray.empty((M,N),a.dtype)\n",
    "    C_gpu = np.empty((M,N),a.dtype)\n",
    "\n",
    "    # call to conv\n",
    "    unit_size = np.dtype(np.int32).itemsize\n",
    "    shared_mem = (M+F-1)*(N+F-1)*unit_size if (M <= 32) and (N <= 32) else (32+F-1)*(32+F-1)*unit_size\n",
    "    # shared_mem = (M+F-1)*(N+F-1)*unit_size if (M <= 32) and (N <= 32) else (100)*(100)*unit_size\n",
    "\n",
    "    start = time.time()\n",
    "    conv(A_buf,K_buf,np.int32(M),np.int32(N),np.int32(F),C_buf,block = (32,32,1),grid = (np.int32(M-1/32)+1,np.int32(N-1/32)+1,1),shared=shared_mem)\n",
    "    gpu_runtime = time.time() - start\n",
    "\n",
    "    # copy data back from GPU\n",
    "    C_gpu = C_buf.get()\n",
    "\n",
    "    return gpu_runtime, C_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== CONFIGURATIONS ========================\n",
      "M:  640\n",
      "N:  640\n",
      "Kernel:  3\n",
      "====================== PART 1-A ==========================\n",
      "--------------------- PYTHON ---------------------------\n",
      "a: \n",
      " [[3 7 7 4 7 6 0 8 ... 1 1 3 2 4 8 7 7]\n",
      " [0 8 7 6 5 9 8 6 ... 9 3 6 0 3 9 8 0]\n",
      " [3 9 8 7 3 2 2 4 ... 6 0 1 6 5 1 9 3]\n",
      " [6 5 7 4 7 5 9 2 ... 4 6 2 9 0 2 0 1]\n",
      " [3 0 1 3 8 6 9 4 ... 3 1 8 1 1 5 0 3]\n",
      " [6 6 3 0 8 4 6 9 ... 1 8 6 9 3 9 9 1]\n",
      " [9 8 2 7 1 4 2 6 ... 7 5 5 7 2 1 5 6]\n",
      " [9 5 5 7 6 6 0 3 ... 5 7 9 7 9 1 9 8]\n",
      " ...\n",
      " [2 1 6 3 7 8 6 8 ... 0 0 3 5 4 4 4 3]\n",
      " [0 0 6 3 9 2 5 0 ... 5 7 2 4 8 9 4 2]\n",
      " [5 9 9 0 8 4 8 2 ... 7 5 8 2 8 9 7 2]\n",
      " [1 8 3 6 7 9 0 2 ... 8 8 7 0 1 8 5 9]\n",
      " [9 5 8 3 4 4 3 9 ... 2 9 5 2 7 2 2 7]\n",
      " [2 4 5 0 1 5 8 1 ... 1 6 6 4 3 8 8 5]\n",
      " [6 9 7 4 0 9 1 3 ... 5 4 0 4 0 4 3 2]\n",
      " [6 6 9 3 4 2 9 3 ... 5 5 8 2 7 1 7 6]]\n",
      "f: \n",
      " [[-1 -2 -1]\n",
      " [ 0  0  0]\n",
      " [ 1  2  1]]\n",
      "c: \n",
      " [[ -8 -23 -28 -24 -25 -31 -31 -21 ... -22 -21 -15  -9 -15 -29 -25  -8]\n",
      " [ -2  -5  -7  -3   9  10   4  11 ... -13  -1   1  -7   1  11   7   6]\n",
      " [ -9   0   5   2   2   5   6   2 ...   3   3  -4 -11   4  25  22   6]\n",
      " [  9  25  27  10 -10 -20 -18 -14 ...   4  -6 -10   7   9   5  14   9]\n",
      " [ -1   2  11  11   3   4   0  -7 ...   1  -5 -10  -7 -13 -26 -25  -9]\n",
      " [-20 -23 -14  -2  12  18  14   7 ...  -4  -9  -4 -10  -4   2  -9 -11]\n",
      " [ -5  -3 -10 -14  -5   4  16  13 ...  -1  -5  -3  -5  -2  10   1 -14]\n",
      " [ 20  13   0  -5  -1   7  13  18 ...  -5  -4  -2   8   0  -7   2   7]\n",
      " ...\n",
      " [  7   8   6   3   2   9  15  15 ...  -4 -15 -12  -8  -7  -3   9  16]\n",
      " [-14 -22 -11   2   5   5   6  17 ... -20 -22 -12  -3 -10 -17 -10  -1]\n",
      " [-10 -14  -5  -1  -6  -7   1   0 ...  -2 -10  -7  10  19   8  -8 -15]\n",
      " [ -4   5   3  -1   5   9   3 -15 ...  13   0   2   4   9  20  12  -5]\n",
      " [  2   5   6  16  22   6 -11 -12 ...  15  12   0  -9  -8  -5  -2   5]\n",
      " [  2  -4  -3   3   2  -4   5  19 ...  -3  12  13   8  10   2   1   9]\n",
      " [-10 -12 -13 -13  -6   2  -1  -2 ... -13  -4  -1  -2   1  11   8  -1]\n",
      " [ 21  31  27  15  13  19  14  10 ...  18  13   8   8   8  11  12   7]]\n",
      "c runtime:  0.07023072242736816\n",
      "----------------------- CUDA ----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jenson\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\math_functions.h: warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\math_functions.h(838): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\math_functions.h(1772): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\math_functions.h(2628): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\math_functions.h(3477): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\math_functions.h(4417): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\math_functions.h(5319): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\math_functions.h(6229): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\math_functions.h(7104): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\math_functions.h(7914): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt/device_functions.h: warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt/device_functions.h(776): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt/device_functions.h(1636): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\crt\\device_double_functions.h: warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\sm_20_intrinsics.h: warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "c:\\program files\\nvidia gpu computing toolkit\\cuda\\v9.0\\include\\sm_20_intrinsics.h(925): warning C4819: The file contains a character that cannot be represented in the current code page (936). Save the file in Unicode format to prevent data loss\r\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "No registered converter was able to produce a C++ rvalue of type unsigned int from this Python object of type numpy.int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f6fb1f0b2117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m640\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m640\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-f6fb1f0b2117>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_M_, _N_, _F_)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------------------- CUDA ----------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mruntime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"c_gpu: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-98285a6f3e9d>\u001b[0m in \u001b[0;36mtest_instance\u001b[1;34m(M, N, F, a, f, c)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mK_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshared_mem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mgpu_runtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pycuda\\driver.py\u001b[0m in \u001b[0;36mfunction_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_launch_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshared\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpost_handlers\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtime_kernel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: No registered converter was able to produce a C++ rvalue of type unsigned int from this Python object of type numpy.int32"
     ]
    }
   ],
   "source": [
    "def main(_M_=5, _N_=5, _F_=3):\n",
    "\n",
    "    print (\"================== CONFIGURATIONS ========================\")\n",
    "    M = _M_ #rows\n",
    "    N = _N_ #columns\n",
    "    F = _F_ #square dim of \"kernel/filter\"\n",
    "\n",
    "    MAKE_PLOT_PART2 = False\n",
    "    MAKE_PLOT_PART3 = False\n",
    "\n",
    "    print (\"M: \", M)\n",
    "    print (\"N: \", N)\n",
    "    print (\"Kernel: \", F)\n",
    "\n",
    "    print(\"====================== PART 1-A ==========================\")\n",
    "    print (\"--------------------- PYTHON ---------------------------\")\n",
    "\n",
    "    a = np.random.randint(0,10, (M,N)).astype(np.int32) #a is matrix which will be convolved\n",
    "\n",
    "    # create an FxF filter of random numbers\n",
    "    # f = np.random.randint(0, 5, (F,F)).astype(np.int32) #f is kernel matrix\n",
    "    # f = np.ones((F,F)).astype(np.int32) #f is kernel matrix\n",
    "    f = np.array([[-1., -2. , -1.], [0., 0., 0.], [1., 2., 1.]]).astype(np.int32)\n",
    "\n",
    "    # mode='same' gives equal (unpadded) input size to OUTPUT size\n",
    "    # boundary='fill' gives zeros around the input as padding\n",
    "    start = time.time()\n",
    "    c = conv2d(a, f, mode='same', boundary='fill')\n",
    "    runtime = time.time() - start\n",
    "\n",
    "    print(\"a: \\n\", a)\n",
    "    print(\"f: \\n\", f)\n",
    "    print(\"c: \\n\", c)\n",
    "    print(\"c runtime: \", runtime)\n",
    "\n",
    "    print(\"----------------------- CUDA ----------------------------\")\n",
    "    runtime, c_gpu = test_instance(M,N,F,a,f,c)\n",
    "\n",
    "    print(\"c_gpu: \\n\", c_gpu)\n",
    "    print(\"c_gpu time: \", runtime)\n",
    "    print(\"c_gpu == c_cpu ? --> \", np.allclose(c_gpu,c))\n",
    "\n",
    "    print(\"====================== PART 1-B ==========================\")\n",
    "    print(\"--------------------- PYTHON ---------------------------\")\n",
    "    M = 3\n",
    "    N = 3\n",
    "    gpu_times = []\n",
    "    gpu_results = []\n",
    "    cpu_times = []\n",
    "    cpu_results = []\n",
    "    for K in range(1,100):\n",
    "        # scale size of input by K (filter remains same as above, constant)\n",
    "        a = np.random.randint(0,10, (M*K,N*K)).astype(np.int32)\n",
    "\n",
    "        start = time.time()\n",
    "        c = conv2d(a, f, mode='same', boundary='fill')\n",
    "        runtime = time.time() - start\n",
    "        cpu_times.append(runtime)\n",
    "        cpu_results.append(c)\n",
    "\n",
    "        runtime, c_gpu = test_instance(M*K,N*K,F,a,f,c)\n",
    "        gpu_times.append(runtime)\n",
    "        gpu_results.append(c_gpu)\n",
    "\n",
    "    # print \"cpu_times: \\n\", cpu_times\n",
    "    # print \"gpu_times: \\n\", gpu_times\n",
    "\n",
    "    flag = True\n",
    "    for x in range(len(gpu_results)):\n",
    "        if(not np.allclose(gpu_results[x],cpu_results[x])):\n",
    "            flag = False\n",
    "    print(\"All GPU results == All CPU results? --> \", flag)\n",
    "\n",
    "    if MAKE_PLOT_PART2:\n",
    "        import matplotlib as mpl\n",
    "        mpl.use('agg')\n",
    "        import matplotlib.pyplot as plt\n",
    "        px = list(xrange(len(cpu_times)))\n",
    "        cx = list(xrange(len(gpu_times)))\n",
    "        # cox = list(xrange(len(cuda_opt_times)))\n",
    "\n",
    "        plt.gcf()\n",
    "        plt.plot(px, cpu_times, color='r', label='python')\n",
    "        plt.plot(cx, gpu_times, color='g', label='CUDA')\n",
    "        # plt.plot(cox, cuda_opt_times, color='b', label='CUDA Optimized')\n",
    "        plt.xlabel('3x3 image * k, with 3x3 kernel')\n",
    "        plt.ylabel('time')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('Matrix Convolution: Python vs. CUDA')\n",
    "        plt.gca().set_xlim((min(px), max(px)))\n",
    "        plt.gca().set_ylim((min(cpu_times)/2, max(gpu_times)*1.2))\n",
    "        plt.savefig('../temp/cuda_scale_image.png')\n",
    "\n",
    "    print(\"====================== PART 1-C ==========================\")\n",
    "    print(\"--------------------- PYTHON ---------------------------\")\n",
    "    M = 300\n",
    "    N = 300\n",
    "    gpu_times = []\n",
    "    gpu_results = []\n",
    "    cpu_times = []\n",
    "    cpu_results = []\n",
    "    for K in range(3,32,2):\n",
    "        # fixed size of input\n",
    "        a = np.random.randint(0,10, (M,N)).astype(np.int32)\n",
    "\n",
    "        # scale size of kernel\n",
    "        F = K\n",
    "        f = np.random.randint(0, 10, (F,F)).astype(np.int32)\n",
    "\n",
    "        start = time.time()\n",
    "        c = conv2d(a, f, mode='same', boundary='fill')\n",
    "        runtime = time.time() - start\n",
    "        cpu_times.append(runtime)\n",
    "        cpu_results.append(c)\n",
    "\n",
    "        runtime, c_gpu = test_instance(M,N,F,a,f,c)\n",
    "        gpu_times.append(runtime)\n",
    "        gpu_results.append(c_gpu)\n",
    "\n",
    "    # print \"cpu_times: \\n\", cpu_times\n",
    "    # print \"gpu_times: \\n\", gpu_times\n",
    "\n",
    "    flag = True\n",
    "    for x in range(len(gpu_results)):\n",
    "        if(not np.allclose(gpu_results[x],cpu_results[x])):\n",
    "            flag = False\n",
    "    print(\"All GPU results == All CPU results? --> \", flag)\n",
    "\n",
    "    if MAKE_PLOT_PART3:\n",
    "        import matplotlib as mpl\n",
    "        mpl.use('agg')\n",
    "        import matplotlib.pyplot as plt\n",
    "        px = list(xrange(len(cpu_times)))\n",
    "        cx = list(xrange(len(gpu_times)))\n",
    "        # cox = list(xrange(len(cuda_opt_times)))\n",
    "\n",
    "        plt.gcf()\n",
    "        plt.plot(px, cpu_times, color='r', label='python')\n",
    "        plt.plot(cx, gpu_times, color='g', label='CUDA')\n",
    "        # plt.plot(cox, cuda_opt_times, color='b', label='CUDA Optimized')\n",
    "        plt.xlabel('300x200 image, with 3kx3k kernel')\n",
    "        plt.ylabel('time')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('Matrix Convolution: Python vs. CUDA')\n",
    "        plt.gca().set_xlim((min(px), max(px)))\n",
    "        plt.gca().set_ylim((min(gpu_times)/2, max(gpu_times)*40))\n",
    "        plt.savefig('../temp/cuda_scale_kernel.png')\n",
    "\n",
    "    print(\"====================== PART 2 ==========================\")\n",
    "    print(\"--------------------- PYTHON ---------------------------\")\n",
    "\n",
    "    from PIL import Image\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ## Use the function below to dynamically change image size.\n",
    "    def create_img(filename, cols , rows):\n",
    "            size = (cols,rows)\n",
    "            im = Image.open(filename).convert('L') #.convert('L') converts the image to grayscale\n",
    "            im = im.resize(size)\n",
    "            return np.array(im).astype(np.int32)\n",
    "\n",
    "    ## To download the image:\n",
    "    ## wget http://heroichollywood.com/wp-content/uploads/2016/06/thrones-002-2.jpg\n",
    "    ##\n",
    "    ## Partially based on:\n",
    "    ## http://www.programcreek.com/python/example/58254/scipy.signal.convolve2d\n",
    "\n",
    "    original_image = create_img(\"../datas/f2.jpg\", 640, 480)\n",
    "\n",
    "    big_smooth = np.array(\n",
    "    [[0., 1., 2., 4., 8., 16., 8., 4., 2., 1., 0.],\n",
    "     [1., 2., 4., 8., 16., 32., 16., 8., 4., 2., 1.],\n",
    "     [2., 4., 8., 16., 32., 64., 32., 16., 8., 4., 2.],\n",
    "     [4., 8., 16., 32., 64., 128., 64., 32., 16., 8., 4.],\n",
    "     [8., 16., 32., 64., 128., 256., 128., 64., 32., 16., 8.],\n",
    "     [16., 32., 64., 128., 256., 512., 256., 128., 64., 32., 16.],\n",
    "     [8., 16., 32., 64., 128., 256., 128., 64., 32., 16., 8.],\n",
    "     [4., 8., 16., 32., 64., 128., 64., 32., 16., 8., 4.],\n",
    "     [2., 4., 8., 16., 32., 64., 32., 16., 8., 4., 2.],\n",
    "     [1., 2., 4., 8., 16., 32., 16., 8., 4., 2., 1.],\n",
    "     [0., 1., 2., 4., 8., 16., 8., 4., 2., 1., 0.],\n",
    "     ]).astype(np.int32)\n",
    "\n",
    "\n",
    "    filters = {\n",
    "                    'identity':np.array([ [0.,0.,0.],[0.,1.,0.],[0.,0.,0.]  ]).astype(np.int32),\n",
    "                    'sharpen':np.array([[0., -1. , 0.], [-1., 5., -1.], [0., -1., 0]]).astype(np.int32),\n",
    "                    'blur':np.array([[1., 1. , 1.], [1., 1., 1.], [1., 1., 1.]]).astype(np.int32),\n",
    "                    'edge_det':np.array([[0., 1. , 0.], [1., -4., 1.], [0., 1., 0]]).astype(np.int32),\n",
    "                    'emboss':np.array([[2., 1. , 0.], [1., 1., -1.], [0., -1., -2]]).astype(np.int32),\n",
    "                    'sob_x':np.array([[-1., 0. , 1.], [-2., 0., 2.], [-1., 0., 1]]).astype(np.int32),\n",
    "                    'sob_y':np.array([[-1., -2. , -1.], [0., 0., 0.], [1., 2., 1.]]).astype(np.int32),\n",
    "                    'smooth_5x5':np.array([[0., 1., 2. , 1., 0.], [1., 4., 8., 4., 1.],[2.,8.,16.,8.,2.],[1.,4.,8.,4.,1.], [0.,1., 2., 1.,0.]]).astype(np.int32),\n",
    "                    'smooth_11x11':big_smooth\n",
    "            }\n",
    "\n",
    "    for filter in filters:\n",
    "        runtime, c_gpu = test_instance(380,612,len(filters[filter][0]),original_image.astype(np.int32),filters[filter],c)\n",
    "        c_cpu = conv2d(original_image.astype(np.int32),filters[filter], mode='same', boundary='fill')\n",
    "        gpu_img = Image.fromarray(c_gpu.astype(np.uint8))\n",
    "        cpu_img = Image.fromarray(c_cpu.astype(np.uint8))\n",
    "        cpu_fn = str(filter) + '_img_cpu_cuda.png'\n",
    "        gpu_fn = str(filter) + '_img_gpu_cuda.png'\n",
    "        cpu_img.save(cpu_fn)\n",
    "        gpu_img.save(gpu_fn)\n",
    "        print (\"Filter: \" + filter)\n",
    "        print (\"c_gpu == c_cpu ? --> \", np.allclose(c_gpu,c_cpu))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain(640,640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
